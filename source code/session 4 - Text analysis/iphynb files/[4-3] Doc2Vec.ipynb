{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Doc2Vec\n",
    "\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "import multiprocessing\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습 4-3-1. 생성한 Doc2Vec 모델을 활용하여, 2000개의 검증 데이터에 대한 감성분석을 수행한다\n",
    "- 로지스틱 회귀 분류기(logistic regression classifier)를 활용하여 각 리뷰를 긍정/부정으로 분류한다\n",
    "- 기계학습 알고리즘의 변수(feature)로는 Doc2Vec의 결과로 생성된 3000차원의 배열(array)를 활용한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lemm = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "LabeledSentence = gensim.models.doc2vec.LabeledSentence\n",
    "\n",
    "class LabeledLineSentence(object):\n",
    "    def __init__(self, doc_list, labels_list):\n",
    "       self.labels_list = labels_list\n",
    "       self.doc_list = doc_list\n",
    "    def __iter__(self):\n",
    "        for idx, doc in enumerate(self.doc_list):\n",
    "            yield LabeledSentence(words=str(doc).split(),tags=[self.labels_list[idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "review_list = []\n",
    "labels_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "files = os.listdir('aclImdb/train/pos')[:1000]\n",
    "for file in files:\n",
    "    review = ''\n",
    "    with open('aclImdb/train/pos/{}'.format(file), 'r', encoding = 'utf-8') as f:\n",
    "        for word in word_tokenize(f.read()):\n",
    "            if lemm.lemmatize(word) not in stop_words:\n",
    "                review += ' ' + word\n",
    "        f.close()\n",
    "    review_list.append(review)\n",
    "    labels_list.append('pos_' + file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "files = os.listdir('aclImdb/train/neg')[:1000]\n",
    "for file in files:\n",
    "    review = ''\n",
    "    with open('aclImdb/train/neg/{}'.format(file), 'r', encoding = 'utf-8') as f:\n",
    "        for word in word_tokenize(f.read()):\n",
    "            if lemm.lemmatize(word) not in stop_words:\n",
    "                review += ' ' + word\n",
    "        f.close()\n",
    "    review_list.append(review)\n",
    "    labels_list.append('neg_' + file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "files = os.listdir('aclImdb/test/pos')[:1000]\n",
    "for file in files:\n",
    "    review = ''\n",
    "    with open('aclImdb/test/pos/{}'.format(file), 'r', encoding = 'utf-8') as f:\n",
    "        for word in word_tokenize(f.read()):\n",
    "            if lemm.lemmatize(word) not in stop_words:\n",
    "                review += ' ' + word\n",
    "        f.close()\n",
    "    review_list.append(review)\n",
    "    labels_list.append('pos_' + file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "files = os.listdir('aclImdb/test/neg')[:1000]\n",
    "for file in files:\n",
    "    review = ''\n",
    "    with open('aclImdb/test/neg/{}'.format(file), 'r', encoding = 'utf-8') as f:\n",
    "        for word in word_tokenize(f.read()):\n",
    "            if lemm.lemmatize(word) not in stop_words:\n",
    "                review += ' ' + word\n",
    "        f.close()\n",
    "    review_list.append(review)\n",
    "    labels_list.append('neg_' + file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "it = LabeledLineSentence(doc_list = review_list, labels_list = labels_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Doc2Vec(size = 3000, window = 10, dm = 0, alpha=0.025, min_alpha=0.025, min_count=5, workers = multiprocessing.cpu_count())\n",
    "\n",
    "model.build_vocab(it)\n",
    "model.train(it, total_examples = 4000, epochs = 20)\n",
    "\n",
    "model.save('partial_Doc2Vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Doc2Vec.load('partial_Doc2Vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.16584291,  0.17257604,  0.11534312, ...,  0.12235802,\n",
       "       -0.14331858, -0.09695759], dtype=float32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.docvecs['pos_0_9.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = np.zeros((2000, 3000))\n",
    "y_train = np.zeros(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "files = os.listdir('aclImdb/train/pos')[:1000]\n",
    "for i in range(1000):\n",
    "    x_train[i] = model.docvecs['pos_' + files[i]]\n",
    "    y_train[i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "files = os.listdir('aclImdb/train/neg')[:1000]\n",
    "for i in range(1000):\n",
    "    x_train[i+1000] = model.docvecs['neg_' + files[i]]\n",
    "    y_train[i+1000] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.16584291  0.17257604  0.11534312 ...,  0.12235802 -0.14331858\n",
      "  -0.09695759]\n",
      " [ 0.05659055  0.06830832  0.00634666 ..., -0.08080529 -0.09798458\n",
      "   0.08413909]\n",
      " [-0.02922071 -0.04415205  0.09993537 ...,  0.01076222 -0.10168039\n",
      "   0.04471296]\n",
      " ..., \n",
      " [-0.03804242 -0.01907704  0.05541293 ...,  0.12065005 -0.12281355\n",
      "  -0.09457269]\n",
      " [ 0.12019002  0.15035789  0.29966483 ..., -0.00458205 -0.07029743\n",
      "  -0.16000935]\n",
      " [ 0.18866819  0.19506106  0.13287938 ...,  0.09275244  0.10702503\n",
      "  -0.05094117]]\n",
      "[ 1.  1.  1. ...,  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "print(x_train)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_test = np.zeros((2000, 3000))\n",
    "y_test = np.zeros(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "files = os.listdir('aclImdb/test/pos')[:1000]\n",
    "for i in range(1000):\n",
    "    x_test[i] = model.docvecs['pos_' + files[i]]\n",
    "    y_test[i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "files = os.listdir('aclImdb/test/neg')[:1000]\n",
    "for i in range(1000):\n",
    "    x_test[i+1000] = model.docvecs['neg_' + files[i]]\n",
    "    y_test[i+1000] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.56103536e-01   3.50850783e-02   1.43749878e-01 ...,   1.60564911e-02\n",
      "   -1.42835140e-01   2.08358653e-02]\n",
      " [ -3.63038443e-02  -5.89871407e-02  -1.28717916e-02 ...,   1.73746347e-01\n",
      "   -2.48201862e-02  -2.21466105e-02]\n",
      " [  4.17211205e-02   1.32784203e-01   1.97908059e-01 ...,   9.37963426e-02\n",
      "   -1.26963094e-01  -1.27556175e-01]\n",
      " ..., \n",
      " [ -2.05457285e-01  -1.50586903e-01  -1.05547339e-01 ...,  -2.06781668e-04\n",
      "   -2.39328876e-01  -1.84734568e-01]\n",
      " [  1.20190024e-01   1.50357887e-01   2.99664825e-01 ...,  -4.58205165e-03\n",
      "   -7.02974349e-02  -1.60009354e-01]\n",
      " [ -4.54753414e-02   3.91226709e-02   1.54420316e-01 ...,   7.63216168e-02\n",
      "    1.85473725e-01   1.47429243e-01]]\n",
      "[ 1.  1.  1. ...,  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "print(x_test)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression()\n",
    "clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.91849999999999998"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습 4-3-2. 생성한 Doc2Vec 모델을 활용하여, 전체 검증 데이터에 대한 감성분석을 수행한다\n",
    "- 로지스틱 회귀 분류기(logistic regression classifier)를 활용하여 각 리뷰를 긍정/부정으로 분류한다\n",
    "- 기계학습 알고리즘의 변수(feature)로는 Doc2Vec의 결과로 생성된 3000차원의 배열(array)를 활용한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "review_list = []\n",
    "labels_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "files = os.listdir('aclImdb/train/pos')\n",
    "for file in files:\n",
    "    review = ''\n",
    "    with open('aclImdb/train/pos/{}'.format(file), 'r', encoding = 'utf-8') as f:\n",
    "        for word in word_tokenize(f.read()):\n",
    "            if lemm.lemmatize(word) not in stop_words:\n",
    "                review += ' ' + word\n",
    "        f.close()\n",
    "    review_list.append(review)\n",
    "    labels_list.append('pos_' + file)\n",
    "    \n",
    "files = os.listdir('aclImdb/train/neg')\n",
    "for file in files:\n",
    "    review = ''\n",
    "    with open('aclImdb/train/neg/{}'.format(file), 'r', encoding = 'utf-8') as f:\n",
    "        for word in word_tokenize(f.read()):\n",
    "            if lemm.lemmatize(word) not in stop_words:\n",
    "                review += ' ' + word\n",
    "        f.close()\n",
    "    review_list.append(review)\n",
    "    labels_list.append('neg_' + file)\n",
    "    \n",
    "files = os.listdir('aclImdb/test/pos')\n",
    "for file in files:\n",
    "    review = ''\n",
    "    with open('aclImdb/test/pos/{}'.format(file), 'r', encoding = 'utf-8') as f:\n",
    "        for word in word_tokenize(f.read()):\n",
    "            if lemm.lemmatize(word) not in stop_words:\n",
    "                review += ' ' + word\n",
    "        f.close()\n",
    "    review_list.append(review)\n",
    "    labels_list.append('pos_' + file)\n",
    "    \n",
    "files = os.listdir('aclImdb/test/neg')\n",
    "for file in files:\n",
    "    review = ''\n",
    "    with open('aclImdb/test/neg/{}'.format(file), 'r', encoding = 'utf-8') as f:\n",
    "        for word in word_tokenize(f.read()):\n",
    "            if lemm.lemmatize(word) not in stop_words:\n",
    "                review += ' ' + word\n",
    "        f.close()\n",
    "    review_list.append(review)\n",
    "    labels_list.append('neg_' + file)\n",
    "    \n",
    "it = LabeledLineSentence(doc_list = review_list, labels_list = labels_list)\n",
    "\n",
    "model = Doc2Vec(size = 3000, window = 10, dm = 0, alpha=0.025, min_alpha=0.025, \\\n",
    "min_count=5, workers = multiprocessing.cpu_count(), )\n",
    "\n",
    "model.build_vocab(it)\n",
    "\n",
    "model.train(it, total_examples = 50000, epochs = 10)\n",
    "model.save('full_Doc2Vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Doc2Vec.load('full_Doc2Vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = np.zeros((25000, 3000))\n",
    "y_train = np.zeros(25000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "files = os.listdir('aclImdb/train/pos')\n",
    "for i in range(12500):\n",
    "    x_train[i] = model.docvecs['pos_' + files[i]]\n",
    "    y_train[i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "files = os.listdir('aclImdb/train/neg')\n",
    "for i in range(12500):\n",
    "    x_train[i+12500] = model.docvecs['neg_' + files[i]]\n",
    "    y_train[i+12500] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_test = np.zeros((25000, 3000))\n",
    "y_test = np.zeros(25000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "files = os.listdir('aclImdb/test/pos')\n",
    "for i in range(12500):\n",
    "    x_test[i] = model.docvecs['pos_' + files[i]]\n",
    "    y_test[i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "files = os.listdir('aclImdb/test/neg')\n",
    "for i in range(12500):\n",
    "    x_test[i+12500] = model.docvecs['neg_' + files[i]]\n",
    "    y_test[i+12500] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression()\n",
    "clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.79012000000000004"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(x_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
