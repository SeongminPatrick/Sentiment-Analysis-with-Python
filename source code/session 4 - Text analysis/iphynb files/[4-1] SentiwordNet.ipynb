{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SentiwordNet\n",
    "- 대규모의 어휘 데이터베이스(lexical database)인 워드넷(wordnet)을 확장해 각 단어의 긍정/부정 척도를 더함\n",
    "- nltk.download()를 통해 sentiwordent corpora를 다운받는다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import sentiwordnet as swn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 단어의 synset (유의어 집합) 찾기\n",
    "- senti_synset()의 결과는 filter object이므로 내용을 보고싶으면 리스트로 변환한다\n",
    "- 품사에 따라 같은 단어라도 다른 유의어 집합이 존재한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<filter at 0xf294080>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swn.senti_synsets('hate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SentiSynset('hate.n.01'), SentiSynset('hate.v.01')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(swn.senti_synsets('hate'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SentiSynset('hate.v.01')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(swn.senti_synsets('hate', 'v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 단어의 긍정/부정 척도\n",
    "- pos_score(): 단어의 긍정 척도\n",
    "- neg_score(): 단어의 부정 척도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(swn.senti_synsets('hate', 'v'))[0].pos_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(swn.senti_synsets('hate', 'v'))[0].neg_score()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 특정 다넝의 긍정/부정 지수를 계산하는 함수 정의\n",
    "- 특정 단어가 속한 유의어 집합의 positive score와 negative score를 모두 합한 후 평균을 낸다\n",
    "- Positive score와 negative score를 tuple로 표현하여 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_sentiment_calculator(word, tag):\n",
    "    pos_score = 0\n",
    "    neg_score = 0\n",
    "    \n",
    "    if 'NN' in tag and len(list(swn.senti_synsets(word, 'n')))>0:\n",
    "        syn_set = list(swn.senti_synsets(word, 'n'))\n",
    "    elif 'VB' in tag and len(list(swn.senti_synsets(word, 'v')))>0:\n",
    "        syn_set = list(swn.senti_synsets(word, 'v'))\n",
    "    elif 'JJ' in tag and len(list(swn.senti_synsets(word, 'a')))>0:\n",
    "        syn_set = list(swn.senti_synsets(word, 'a'))\n",
    "    elif 'RB' in tag and len(list(swn.senti_synsets(word, 'r')))>0:\n",
    "        syn_set = list(swn.senti_synsets(word, 'r'))\n",
    "    else:\n",
    "        return (0,0)\n",
    "    \n",
    "    for syn in syn_set:\n",
    "        pos_score += syn.pos_score()\n",
    "        neg_score += syn.neg_score()\n",
    "    return (pos_score/len(syn_set), neg_score/len(syn_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('love', 'NN')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(['love'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.22916666666666666, 0.0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_sentiment_calculator('love', 'NN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.625, 0.03125)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_sentiment_calculator('love', 'VB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 특정 문장의 긍정/부정 지수 계산하기\n",
    "- 문장을 토큰화한 후 품사 태깅을 한다\n",
    "- 각 토큰의 부정 지수와 긍정 지수를 모두 합한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent = 'I hate you'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP'), ('hate', 'VBP'), ('you', 'PRP')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize(sent)\n",
    "pos_tags = nltk.pos_tag(tokens)\n",
    "pos_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_score = 0\n",
    "neg_score = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.75\n"
     ]
    }
   ],
   "source": [
    "for word, tag in pos_tags:\n",
    "    pos_score += word_sentiment_calculator(word, tag)[0]\n",
    "    neg_score += word_sentiment_calculator(word, tag)[1]\n",
    "print(pos_score)\n",
    "print(neg_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문장의 감성 지수를 계산하기 위한 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentence_sentiment_calculator(sent):\n",
    "    tokens =  nltk.word_tokenize(sent)\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    \n",
    "    pos_score = 0\n",
    "    neg_score = 0\n",
    "    for word, tag in pos_tags:\n",
    "        pos_score += word_sentiment_calculator(word, tag)[0]\n",
    "        neg_score += word_sentiment_calculator(word, tag)[1]\n",
    "    return (pos_score, neg_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습 4-1-1. 센티워드넷을 활용한 문장 감성분석\n",
    "- 아래 문장의 감성분석을 수행해 본다\n",
    "    - 문장1: “In the Echo Dot, Amazon has created a near perfect blend of hardware and software.”\n",
    "    - 문장2: “The author does a good job of presenting a wide range of psychological traps and irrational tendencies to which humans fall prey”\n",
    "    - 문장3: “Pulp Fiction is inane, self-indulgent, and bloated”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_sentiment_calculator(word, tag):\n",
    "    pos_score = 0\n",
    "    neg_score = 0\n",
    "    \n",
    "    if 'NN' in tag and len(list(swn.senti_synsets(word, 'n')))>0:\n",
    "        syn_set = list(swn.senti_synsets(word, 'n'))\n",
    "    elif 'VB' in tag and len(list(swn.senti_synsets(word, 'v')))>0:\n",
    "        syn_set = list(swn.senti_synsets(word, 'v'))\n",
    "    elif 'JJ' in tag and len(list(swn.senti_synsets(word, 'a')))>0:\n",
    "        syn_set = list(swn.senti_synsets(word, 'a'))\n",
    "    elif 'RB' in tag and len(list(swn.senti_synsets(word, 'r')))>0:\n",
    "        syn_set = list(swn.senti_synsets(word, 'r'))\n",
    "    else:\n",
    "        return (0,0)\n",
    "    \n",
    "    for syn in syn_set:\n",
    "        pos_score += syn.pos_score()\n",
    "        neg_score += syn.neg_score()\n",
    "    return (pos_score/len(syn_set), neg_score/len(syn_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentence_sentiment_calculator(sent):\n",
    "    tokens =  nltk.word_tokenize(sent)\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    \n",
    "    pos_score = 0\n",
    "    neg_score = 0\n",
    "    for word, tag in pos_tags:\n",
    "        pos_score += word_sentiment_calculator(word, tag)[0]\n",
    "        neg_score += word_sentiment_calculator(word, tag)[1]\n",
    "    return (pos_score, neg_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent1 = 'In the Echo Dot, Amazon has created a near perfect blend of hardware and software.'\n",
    "sent2 = 'The author does a good job of presenting a wide range of psychological traps and irrational tendencies to which humans fall prey'\n",
    "sent3 = 'Pulp Fiction is inane, self-indulgent, and bloated'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1.1239035087719298, 0.2609649122807018)\n",
      "(1.1782757173382172, 0.882884043040293)\n",
      "(0.7788461538461539, 0.9942307692307693)\n"
     ]
    }
   ],
   "source": [
    "print(sentence_sentiment_calculator(sent1))\n",
    "print(sentence_sentiment_calculator(sent2))\n",
    "print(sentence_sentiment_calculator(sent3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습 4-1-2. 센티워드넷을 활용한 영화 리뷰 감성분석 (1)\n",
    "- 실습 1-3-4에서 수집했던 영화 ‘다크 나이트’의 첫 번째 리뷰를 불러와 감성분석을 수행해 본다\n",
    "- 센티워드넷을 활용한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('result-1-3-4.txt', 'r', encoding = 'utf-8') as f:\n",
    "    review = f.readline()\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11.405738623218884, 6.483186195266968)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_sentiment_calculator(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습 4-1-3. 센티워드넷을 활용한 영화 리뷰 감성분석 (2)\n",
    "- 실습 1-3-4에서 수집했던 영화 ‘다크 나이트’의 모든 리뷰의 감성분석을 수행한다\n",
    "- 모든 리뷰의 감성 지수를 2차원 NumPy 배열에 저장한 후 긍정/부정 지수의 평균을 계산한다\n",
    "- np.mean() 함수를 활용한다(axis 인자값을 활용할 것)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('result-1-3-4.txt', 'r', encoding = 'utf-8') as f:\n",
    "    all_reviews = f.readlines()\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores = []\n",
    "for review in all_reviews:\n",
    "    scores.append(sentence_sentiment_calculator(review))\n",
    "scores = np.array(scores)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mean_scores = np.mean(scores, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 13.4369763 ,  10.19789012])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습 4-1-4. 센티워드넷을 활용한 영화 리뷰 감성분석 (3)\n",
    "- 실습 1-3-5에서 수집했던 임의의 영화의 모든 리뷰의 감성분석을 수행한다\n",
    "- 모든 리뷰의 감성 지수를 2차원 NumPy 배열에 저장한다\n",
    "- 이를 open()함수를 활용해 텍스트 파일에 저장한다\n",
    "    - 각 리뷰당 한 줄에 출력되고, 긍정과 부정 지수의 구분은 탭(tab)으로 한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('result-1-3-5-inception.txt', 'r', encoding = 'utf-8') as f:\n",
    "    all_reviews = f.readlines()\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores = []\n",
    "for review in all_reviews:\n",
    "    scores.append(sentence_sentiment_calculator(review))\n",
    "scores = np.array(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('result-4-1-4.txt', 'w', encoding = 'utf-8') as f:\n",
    "    for score in scores:\n",
    "        f.write(str(score[0]) + '\\t' + str(score[1]) + '\\r')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습 4-1-5. 센티워드넷을 활용한 IMDb Large Movie Review Dataset 감성분석 (1)\n",
    "- IMDb Large Movie Review Dataset에서 첫 번째 긍정적인 학습 리뷰 데이터를 불러와 감성 분석을 해본다\n",
    "- [aclImdb] > [train] > [pos]> 0_9.txt의 데이터를 분석해 본다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "files = os.listdir('aclImdb/train/pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "first_file = files[0]\n",
    "with open('aclImdb/train/pos/{}'.format(first_file), 'r', encoding = 'utf-8') as f:\n",
    "    review = f.read()\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as \"Teachers\". My 35 years in the teaching profession lead me to believe that Bromwell High's satire is much closer to reality than is \"Teachers\". The scramble to survive financially, the insightful students who can see right through their pathetic teachers' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. A classic line: INSPECTOR: I'm here to sack one of your teachers. STUDENT: Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far fetched. What a pity that it isn't!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5.110205982026105, 3.502637966031258)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(review)\n",
    "sentence_sentiment_calculator(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습 4-1-6. 센티워드넷을 활용한 IMDb Large Movie Review Dataset 감성분석 (2)\n",
    "- IMDb Large Movie Review Dataset에서 긍정적인 학습 리뷰 데이터 10개와 부정적인 학습 리뷰 데이터 10개를 불러와 감성 분석을 해본다\n",
    "- 총 20개 중에 정확도(accuracy)가 몇이나 되는지 확인해 본다\n",
    "    - 긍정 척도가 부정 척도보다 높으면 positive로, 부정 척도가 긍정 척도보다 높으면 negative로 분류한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "pos_files = os.listdir('aclImdb/train/pos')[:10]\n",
    "neg_files = os.listdir('aclImdb/train/neg')[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "actual = [1] *10 + [0]*10 \n",
    "predicted = []\n",
    "\n",
    "for file in pos_files:\n",
    "    with open('aclImdb/train/pos/{}'.format(file), 'r', encoding = 'utf-8') as f:\n",
    "        scores = sentence_sentiment_calculator(f.read())\n",
    "        \n",
    "        if scores[0] >= scores[1]:\n",
    "            predicted.append(1)\n",
    "        else:\n",
    "            predicted.append(0)\n",
    "        f.close()\n",
    "\n",
    "for file in neg_files:\n",
    "    with open('aclImdb/train/neg/{}'.format(file), 'r', encoding = 'utf-8') as f:\n",
    "        scores = sentence_sentiment_calculator(f.read())\n",
    "        \n",
    "        if scores[0] >= scores[1]:\n",
    "            predicted.append(1)\n",
    "        else:\n",
    "            predicted.append(0)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct = 0\n",
    "incorrect = 0\n",
    "for i in range(20):\n",
    "    if actual[i] == predicted[i]:\n",
    "        correct += 1\n",
    "    else:\n",
    "        incorrect += 1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(actual)\n",
    "print(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Number of correct instances: ', correct)\n",
    "print('Number of incorrect instances: ', incorrect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습 4-1-7. 센티워드넷을 활용한 IMDb Large Movie Review Dataset 감성분석 (3)\n",
    "- IMDb Large Movie Review Dataset에서 긍정적인 검증 리뷰 데이터와 부정적인 검증 리뷰 데이터를 1000개씩 불러와 감성 분석을 해본다\n",
    "- 총 2000개 중에 정확도(accuracy)가 얼마나 되는지 살펴본다\n",
    "    - 긍정 척도가 부정 척도보다 높으면 positive로, 부정 척도가 긍정 척도보다 높으면 negative로 분류한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "pos_files = os.listdir('aclImdb/test/pos')[:1000]\n",
    "neg_files = os.listdir('aclImdb/test/neg')[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "actual = [1] *1000 + [0]*1000 \n",
    "predicted = []\n",
    "\n",
    "for file in pos_files:\n",
    "    with open('aclImdb/test/pos/{}'.format(file), 'r', encoding = 'utf-8') as f:\n",
    "        scores = sentence_sentiment_calculator(f.read())\n",
    "        \n",
    "        if scores[0] >= scores[1]:\n",
    "            predicted.append(1)\n",
    "        else:\n",
    "            predicted.append(0)\n",
    "        f.close()\n",
    "\n",
    "for file in neg_files:\n",
    "    with open('aclImdb/test/neg/{}'.format(file), 'r', encoding = 'utf-8') as f:\n",
    "        scores = sentence_sentiment_calculator(f.read())\n",
    "        \n",
    "        if scores[0] >= scores[1]:\n",
    "            predicted.append(1)\n",
    "        else:\n",
    "            predicted.append(0)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct = 0\n",
    "incorrect = 0\n",
    "for i in range(2000):\n",
    "    if actual[i] == predicted[i]:\n",
    "        correct += 1\n",
    "    else:\n",
    "        incorrect += 1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of correct instances:  1425\n",
      "Number of incorrect instances:  575\n"
     ]
    }
   ],
   "source": [
    "print('Number of correct instances: ', correct)\n",
    "print('Number of incorrect instances: ', incorrect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습 4-1-8. 센티워드넷을 활용한 IMDb Large Movie Review Dataset 감성분석 (4)\n",
    "- IMDb Large Movie Review Dataset에서 긍정적인 검증 리뷰 데이터와 부정적인 검증 리뷰 데이터 전체를 불러와 감성 분석을 해본다\n",
    "- 총 25000개 중에 정확도(accuracy)가 얼마나 되는지 살펴본다\n",
    "    - 긍정 척도가 부정 척도보다 높으면 positive로, 부정 척도가 긍정 척도보다 높으면 negative로 분류한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "pos_files = os.listdir('aclImdb/test/pos')\n",
    "neg_files = os.listdir('aclImdb/test/neg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "actual = [1] *12500 + [0]*12500 \n",
    "predicted = []\n",
    "\n",
    "for file in pos_files:\n",
    "    with open('aclImdb/test/pos/{}'.format(file), 'r', encoding = 'utf-8') as f:\n",
    "        scores = sentence_sentiment_calculator(f.read())\n",
    "        \n",
    "        if scores[0] >= scores[1]:\n",
    "            predicted.append(1)\n",
    "        else:\n",
    "            predicted.append(0)\n",
    "        f.close()\n",
    "\n",
    "for file in neg_files:\n",
    "    with open('aclImdb/test/neg/{}'.format(file), 'r', encoding = 'utf-8') as f:\n",
    "        scores = sentence_sentiment_calculator(f.read())\n",
    "        \n",
    "        if scores[0] >= scores[1]:\n",
    "            predicted.append(1)\n",
    "        else:\n",
    "            predicted.append(0)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct = 0\n",
    "incorrect = 0\n",
    "for i in range(25000):\n",
    "    if actual[i] == predicted[i]:\n",
    "        correct += 1\n",
    "    else:\n",
    "        incorrect += 1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of correct instances:  17057\n",
      "Number of incorrect instances:  7943\n"
     ]
    }
   ],
   "source": [
    "print('Number of correct instances: ', correct)\n",
    "print('Number of incorrect instances: ', incorrect)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
